# Metrics and Monitoring

This document describes the comprehensive metrics and Prometheus integration for the distributed LLM system.

## Overview

The distributed LLM system includes comprehensive metrics collection using Prometheus to monitor:
- Node resources and health
- Network performance and connectivity 
- Inference request performance
- Model loading and layer allocation
- System resource utilization

## Metrics Categories

### Node Metrics
- `distributed_llm_node_status`: Node status (0=offline, 1=online, 2=busy, 3=unknown)
- `distributed_llm_node_uptime_seconds`: Node uptime in seconds
- `distributed_llm_node_resources`: Node resource information (CPU cores, memory, max layers)

### Network Metrics
- `distributed_llm_network_connections`: Number of active network connections
- `distributed_llm_network_messages_total`: Total network messages sent/received by direction and type
- `distributed_llm_network_latency_seconds`: Network request latency histogram

### Inference Metrics
- `distributed_llm_inference_requests_total`: Total inference requests by model and status
- `distributed_llm_inference_latency_seconds`: Inference request latency histogram
- `distributed_llm_inference_tokens_generated`: Total tokens generated by model

### Model Metrics
- `distributed_llm_models_loaded`: Number of loaded models
- `distributed_llm_model_size_bytes`: Size of loaded models in bytes
- `distributed_llm_layers_allocated`: Number of layers allocated per node/model

### System Metrics
- `distributed_llm_system_memory_usage_bytes`: System memory usage
- `distributed_llm_system_cpu_usage_percent`: System CPU usage percentage
- `distributed_llm_system_goroutines`: Number of active goroutines

### Health Metrics
- `distributed_llm_health_check_total`: Total health checks by status
- `distributed_llm_health_check_latency_seconds`: Health check latency

## Configuration

### Agent Configuration
The agent exposes metrics on port 9090 by default:

```bash
./agent --node-id=node1 --metrics-port=9090
```

### Kubernetes Deployment
For Kubernetes deployments, add these annotations to your pods:

```yaml
metadata:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    prometheus.io/path: "/metrics"
```

## Local Development

### Start Monitoring Stack
```bash
# Start Prometheus
make metrics

# Start Grafana
make grafana

# Start both (full monitoring stack)
make monitoring
```

### Stop Monitoring Stack
```bash
# Stop individual services
make metrics-stop
make grafana-stop

# Stop entire stack
make monitoring-stop
```

### Check Metrics
```bash
# Check if metrics endpoint is working
make metrics-check

# Or manually check
curl http://localhost:9090/metrics
```

## Prometheus Configuration

### Static Configuration (prometheus.yml)
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'distributed-llm-agents'
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: /metrics
    scrape_interval: 5s
```

### Kubernetes Service Discovery
The system supports automatic service discovery in Kubernetes using pod annotations.

## Grafana Dashboard

A comprehensive Grafana dashboard is provided in `deployments/grafana/dashboard.json` with:

- Node status overview
- Network message rates
- Resource utilization (CPU, memory, layers)
- Network latency percentiles
- Inference request rates and latency
- Token generation rates
- System metrics (memory, CPU, goroutines)
- Health check status

### Import Dashboard
1. Open Grafana (http://localhost:3000)
2. Login with admin/admin
3. Import dashboard from `deployments/grafana/dashboard.json`

## Production Deployment

### Kubernetes Prometheus Setup
Deploy Prometheus in your cluster:

```bash
kubectl apply -f deployments/prometheus/prometheus-k8s.yaml
```

This creates:
- Prometheus deployment with service discovery
- ServiceAccount with appropriate RBAC permissions
- PVC for metrics storage
- Service for Prometheus UI access

### Security Considerations
- Metrics endpoints expose system information
- Consider network policies to restrict access
- Use authentication for Prometheus/Grafana in production
- Monitor metrics storage usage and retention

## Troubleshooting

### Common Issues

1. **Metrics not appearing**
   - Check if agent is running with correct `--metrics-port`
   - Verify metrics endpoint: `curl http://localhost:9090/metrics`
   - Check Prometheus configuration and targets

2. **High cardinality warnings**
   - Monitor label combinations to avoid cardinality explosion
   - Consider aggregating or sampling high-frequency metrics

3. **Performance impact**
   - Metrics collection has minimal overhead (~1-2% CPU)
   - Adjust scrape intervals if needed
   - Monitor prometheus storage requirements

### Debug Commands
```bash
# Check metrics endpoint
curl -s http://localhost:9090/metrics | grep distributed_llm

# Check health endpoint  
curl http://localhost:9090/health

# View Prometheus targets (if running locally)
curl http://localhost:9091/api/v1/targets

# Check agent logs for metrics errors
journalctl -u distributed-llm-agent | grep -i metrics
```

## Metrics Best Practices

1. **Label Usage**
   - Use consistent label names across metrics
   - Keep cardinality reasonable (< 10k series per metric)
   - Avoid high-cardinality labels like timestamps or UUIDs

2. **Naming Conventions**
   - Follow Prometheus naming conventions
   - Use descriptive metric names with units
   - Group related metrics with common prefixes

3. **Alerting**
   - Set up alerts for critical metrics (node status, high latency)
   - Use rate() for counter metrics in alerts
   - Consider alert fatigue when setting thresholds

## Integration Points

The metrics are integrated at several levels:

- **Network Layer**: P2P network events, message passing, connection management
- **Agent Layer**: Resource broadcasting, node lifecycle
- **gRPC Layer**: Inference request metrics via interceptors  
- **System Layer**: Periodic collection of system metrics
- **Health Layer**: Health check timing and status
